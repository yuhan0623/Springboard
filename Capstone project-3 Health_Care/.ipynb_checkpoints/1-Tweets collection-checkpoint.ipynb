{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Health Tweets -- Data Collection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is to scrap tweets data based on health keywords from Twitter API using tweepy. \n",
    "\n",
    "Specifically, I'll be walking through:\n",
    "\n",
    "  1. **Getting the data** \n",
    "    - Collect the data from Twitter APIs\n",
    "  2. **Cleaning the data** \n",
    "    - Make text all lower case\n",
    "    - Remove punctuation\n",
    "    - Remove numerical values\n",
    "    - Remove common non-sensical text (/n)\n",
    "    - Tokenize text\n",
    "    - Remove stop words\n",
    "    - Stemming / lemmatization\n",
    "    - Parts of speech tagging\n",
    "    - Create bi-grams or tri-grams\n",
    "    - Deal with typos\n",
    "  3. **Organizing the data**\n",
    "    - Corpos\n",
    "    - Document-Term Matrix\n",
    "\n",
    "## Problem Statement\n",
    "Health is one of  the most important things in our life. But different people have different concerns about their health. \n",
    "\n",
    "The goal of this project is to know what type of health people are concerned about more, and apply the sentiment analysis to check how negative they think about their health, and how much time they cost on their health.\n",
    "\n",
    "## Getting the data\n",
    "I collected the data from Twitter APIs. Keywords that could be used to filter is https://figshare.com/articles/dataset/List_of_Health_Keywords/1084358/1 and covid-19 from 2017 to 2020. I label them into a few groups based on the keywords like mental health concern, flu, dental, covid-19 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load python packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter credentials\n",
    "consumer_key = 'lBzNFpVdxWRFYfW7XxO7zP8WS'\n",
    "consumer_secret = 'LjGR3qSETMxqb9eyFONFQJyA85cfvSeqdq52paYV2KAfd1xuFH'\n",
    "access_key = '1324831202449199109-rwCWdxMtMS33Ry1SMIlbXRlsABvgYd'\n",
    "access_secret = 'rMl5JZNBpK9Z0rWzhHoHzV1AgOSGBkQg5SwDJNbQzKDj2'\n",
    "# Create the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Set the access token and access token secret\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the keywords from txt to a list\n",
    "txt_list = []\n",
    "\n",
    "with open('keywords.txt', \"r\") as f:\n",
    "    txt_list = f.read().split()\n",
    "\n",
    "keywords = [word.strip(',') for word in txt_list]\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraptweets(search_words, date_since ,date_until, numTweets, numRuns):\n",
    "\n",
    "    ## Arguments:\n",
    "    # search_words -> define a string of keywords for this function to extract\n",
    "    # date_since -> define a date from which to start extracting the tweets \n",
    "    # numTweets -> number of tweets to extract per run\n",
    "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
    "    ##\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['text', 'user_name', 'user_description', 'user_location','user_created_date','user_followers_count', \n",
    "                                        'user_friends_count', 'user_favourites','user_verified','date', 'hashtags', \n",
    "                                        'source','totaltweets'])\n",
    "\n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    for i in range(0, numRuns):\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until = date_until, \n",
    "                               tweet_mode='extended', wait_on_rate_limit=True, wait_on_rate_limit_notify=True).items(numTweets)\n",
    "\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "\n",
    "        # Obtain the following info (methods to call them out):\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "\n",
    "            # Pull the values\n",
    "            user_name = tweet.user.screen_name\n",
    "            user_description = tweet.user.description\n",
    "            user_location = tweet.user.location\n",
    "            user_created_date = tweet.user.created_at\n",
    "            user_followers_count = tweet.user.followers_count\n",
    "            user_friends_count = tweet.user.friends_count\n",
    "            user_favourites = tweet.favorite_count\n",
    "            user_verified = tweet.user.verified\n",
    "            date = tweet.created_at\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            source = tweet.source\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            # Add the 6 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [text, user_name, user_description, user_location, user_created_date, user_followers_count, \n",
    "                          user_friends_count, user_favourites, user_verified, date, hashtags, source, totaltweets]\n",
    "\n",
    "            # Append to dataframe - db_tweets \n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "            \n",
    "        print('sleep for 15 min')    \n",
    "        time.sleep(900) #15 minute sleep time\n",
    "\n",
    "    return db_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a limitation about amount of keywords can be searched in the same time\n",
    "# I need seperate the keywords to a few strings\n",
    "n = len(keywords)//30\n",
    "search_words_all =[]\n",
    "for i in range(n):\n",
    "    start = i*30\n",
    "    end = (i+1)*30\n",
    "    search_words_all.append(' OR '.join(keywords[start:end]))\n",
    "last = len(keywords) - len(keywords)//30 * 30\n",
    "search_words_all.append(' OR '.join(keywords[-last:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up searching time\n",
    "dates = [\"2020-11-03\",\"2020-11-04\", \"2020-11-05\", \"2020-11-06\", \"2020-11-07\", \"2020-11-08\", \"2020-11-09\", \"2020-11-10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up searching time\n",
    "dates = [\"2020-11-06\", \"2020-11-07\", \"2020-11-08\", \"2020-11-09\", \"2020-11-10\",\"2020-11-11\",\"2020-11-12\", \"2020-11-13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numTweets = 2500\n",
    "numRuns = 3\n",
    "for date in range(len(dates)-1):\n",
    "    # Begin scraping the tweets individually:\n",
    "    date_since = dates[date]\n",
    "    date_until = dates[date+1]\n",
    "    print(date_since, date_until)\n",
    "    db_tweets = pd.DataFrame(columns = ['text', 'user_name', 'user_description', 'user_location','user_created_date','user_followers_count', \n",
    "                                        'user_friends_count', 'user_favourites','user_verified','date', 'hashtags', \n",
    "                                        'source','totaltweets'])\n",
    "    # We will time how long it takes to scrape tweets for each run:\n",
    "    start_run = time.time()\n",
    "    print(datetime.now())\n",
    "    for word in search_words_all:\n",
    "        search_words = word\n",
    "        db_tweets = db_tweets.append(scraptweets(search_words, date_since, date_until, numTweets, numRuns))\n",
    "    # Once all runs have completed, save them to a single csv file: \n",
    "    # Obtain timestamp in a readable format:\n",
    "    \n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_health_tweets.csv'\n",
    "\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    # Run ended:\n",
    "    end_run = time.time()\n",
    "    print(datetime.now())\n",
    "    duration_run = round(end_run-start_run, 2)\n",
    "    print('time take for {} run to complete is {}'.format(date, duration_run))\n",
    "    print(date_since, 'to', date_until, 'has completed!')\n",
    "print('WHole Scraping has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
